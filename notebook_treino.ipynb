{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731969e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.19.0\n",
      "StatsForecast Version: 2.0.1\n",
      "Usando arquivo de dados: dados/preco_petroleo.csv\n",
      "Arquivos de modelo serão salvos em: c:\\Users\\ReDragon\\Desktop\\TECH_CHALLENGE_FASE_4\\STREAMLIT_APP\n",
      "Classes de Pipeline definidas.\n",
      "Função calcula_erro_notebook definida.\n",
      "Dados filtrados para os últimos 10 anos (a partir de 2014-11-04). Formato: (2961, 2)\n",
      "\n",
      "--- Treinando Modelo AutoARIMAX (StatsForecast) para salvar (10 anos com features) ---\n",
      ">>> Scaler para features exógenas do ARIMAX salvo como './scaler_exog_arima.pkl' <<<\n",
      "Ajustando AutoARIMAX com season_length=0. As features exógenas estão no DataFrame de entrada.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ReDragon\\AppData\\Local\\Temp\\ipykernel_4324\\212439288.py:67: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_copy[self.value_col].ffill(inplace=True); df_copy[self.value_col].bfill(inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoARIMAX (10 anos com features) ajustado.\n",
      "Saving StatsForecast object of size 403.41KB.\n",
      "StatsForecast object saved\n",
      ">>> Modelo AutoARIMAX (StatsForecast, 10 anos com features) salvo como './sarima_model_sf.pkl' <<<\n",
      "\n",
      "--- Gerando Features ARIMAX para LSTM Híbrido (baseado no modelo de 10 anos) ---\n",
      "Features ARIMAX (10 anos) geradas e revertidas para escala original. Formato: (3654, 2)\n",
      "\n",
      "--- Preparando Dados Combinados e Features Sazonais para LSTM Híbrido (10 anos) ---\n",
      "Dados combinados com features sazonais para LSTM. Formato: (3654, 6)\n",
      "\n",
      "--- Treinamento do Modelo LSTM Híbrido (10 anos com Features Sazonais) ---\n",
      ">>> Scaler LSTM (Híbrido com Features, 10 anos) salvo como './scaler.pkl' <<<\n",
      "LSTM usará seq_length=90 e num_features=6\n",
      "Formato de X_train_lstm_tuned (multivariado): (2852, 90, 6)\n",
      "\n",
      "Iniciando treinamento do modelo LSTM híbrido com features sazonais (10 anos)...\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ReDragon\\AppData\\Local\\Temp\\ipykernel_4324\\212439288.py:67: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_copy[self.value_col].ffill(inplace=True); df_copy[self.value_col].bfill(inplace=True)\n",
      "c:\\Users\\ReDragon\\Desktop\\TECH_CHALLENGE_FASE_4\\STREAMLIT_APP\\venv\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 107ms/step - loss: 0.0643 - val_loss: 0.0082\n",
      "Epoch 2/200\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 91ms/step - loss: 0.0051 - val_loss: 9.8872e-04\n",
      "Epoch 3/200\n",
      "\u001b[1m 6/36\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 82ms/step - loss: 0.0032"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 336\u001b[0m\n\u001b[0;32m    334\u001b[0m early_stopping_10a \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    335\u001b[0m validation_data_lstm_10a \u001b[38;5;241m=\u001b[39m (X_val_lstm_10a_tuned, y_val_lstm_10a_tuned) \u001b[38;5;28;01mif\u001b[39;00m X_val_lstm_10a_tuned\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 336\u001b[0m history_lstm_10a_tuned \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_lstm_hibrido_10a_tuned\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train_lstm_10a_tuned\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_lstm_10a_tuned\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs_10a_tuned\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size_10a_tuned\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data_lstm_10a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping_10a\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvalidation_data_lstm_10a\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTreinamento LSTM (10 anos com features) concluído.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    341\u001b[0m model_lstm_h5_path_10a_tuned \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_directory, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlstm_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ReDragon\\Desktop\\TECH_CHALLENGE_FASE_4\\STREAMLIT_APP\\venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\ReDragon\\Desktop\\TECH_CHALLENGE_FASE_4\\STREAMLIT_APP\\venv\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:371\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[0;32m    370\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 371\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    372\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[0;32m    373\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[1;32mc:\\Users\\ReDragon\\Desktop\\TECH_CHALLENGE_FASE_4\\STREAMLIT_APP\\venv\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:219\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    217\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[0;32m    218\u001b[0m     ):\n\u001b[1;32m--> 219\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[0;32m    221\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ReDragon\\Desktop\\TECH_CHALLENGE_FASE_4\\STREAMLIT_APP\\venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\ReDragon\\Desktop\\TECH_CHALLENGE_FASE_4\\STREAMLIT_APP\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\ReDragon\\Desktop\\TECH_CHALLENGE_FASE_4\\STREAMLIT_APP\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ReDragon\\Desktop\\TECH_CHALLENGE_FASE_4\\STREAMLIT_APP\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ReDragon\\Desktop\\TECH_CHALLENGE_FASE_4\\STREAMLIT_APP\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\ReDragon\\Desktop\\TECH_CHALLENGE_FASE_4\\STREAMLIT_APP\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\ReDragon\\Desktop\\TECH_CHALLENGE_FASE_4\\STREAMLIT_APP\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ReDragon\\Desktop\\TECH_CHALLENGE_FASE_4\\STREAMLIT_APP\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1686\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1688\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1689\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1690\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1691\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1692\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1693\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1694\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1695\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1696\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1697\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1698\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1702\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1703\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\ReDragon\\Desktop\\TECH_CHALLENGE_FASE_4\\STREAMLIT_APP\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CÉLULA 1: Imports e Configurações Iniciais\n",
    "# ==============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import joblib\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Imports para Modelagem\n",
    "import statsforecast # Para verificar a versão\n",
    "from statsforecast import StatsForecast\n",
    "from statsforecast.models import AutoARIMA\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from statsmodels.tsa.stattools import adfuller \n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf \n",
    "\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"StatsForecast Version: {statsforecast.__version__}\")\n",
    "\n",
    "# --- Configuração de Caminhos ---\n",
    "output_directory = \"./\" \n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "file_path_data_notebook = 'dados/preco_petroleo.csv' \n",
    "print(f\"Usando arquivo de dados: {file_path_data_notebook}\")\n",
    "print(f\"Arquivos de modelo serão salvos em: {os.path.abspath(output_directory)}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# CÉLULA 2: Definição das Classes do Pipeline e Função de Erro\n",
    "# ==============================================================================\n",
    "class PrepareData(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, date_col='Data', value_col='Value'):\n",
    "        self.date_col = date_col\n",
    "        self.value_col = value_col\n",
    "    def fit(self, df, y=None): return self\n",
    "    def transform(self, df):\n",
    "        df_copy = df.copy()\n",
    "        if 'Date' in df_copy.columns and self.date_col == 'Data': df_copy.rename(columns={'Date': 'Data'}, inplace=True)\n",
    "        if 'Preço - petróleo bruto - Brent (FOB)' in df_copy.columns and self.value_col == 'Value': df_copy.rename(columns={'Preço - petróleo bruto - Brent (FOB)': 'Value'}, inplace=True)\n",
    "        if self.date_col not in df_copy.columns: raise ValueError(f\"Coluna de data '{self.date_col}' não encontrada.\")\n",
    "        if self.value_col not in df_copy.columns: raise ValueError(f\"Coluna de valor '{self.value_col}' não encontrada.\")\n",
    "        df_copy[self.date_col] = pd.to_datetime(df_copy[self.date_col])\n",
    "        df_copy.set_index(self.date_col, inplace=True); df_copy[self.value_col] = df_copy[self.value_col].astype(float)\n",
    "        df_copy.dropna(subset=[self.value_col], inplace=True); df_copy.sort_index(inplace=True, ascending=True)\n",
    "        return df_copy\n",
    "\n",
    "class FillNANValues(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, value_col='Value', new_value_col='y', new_date_col='ds'):\n",
    "        self.value_col, self.new_value_col, self.new_date_col = value_col, new_value_col, new_date_col\n",
    "    def fit(self, df, y=None): return self\n",
    "    def transform(self, df):\n",
    "        df_copy = df.copy()\n",
    "        if not isinstance(df_copy.index, pd.DatetimeIndex): raise ValueError(\"DatetimeIndex esperado.\")\n",
    "        start_date, end_date = df_copy.index.min(), df_copy.index.max()\n",
    "        if pd.isna(start_date) or pd.isna(end_date) or df_copy.empty: return pd.DataFrame(columns=[self.new_date_col, self.new_value_col])\n",
    "        df_copy = df_copy.reindex(pd.date_range(start=start_date, end=end_date, freq='D'))\n",
    "        df_copy[self.value_col].ffill(inplace=True); df_copy[self.value_col].bfill(inplace=True)\n",
    "        df_copy.dropna(subset=[self.value_col], inplace=True)\n",
    "        if df_copy.empty: return pd.DataFrame(columns=[self.new_date_col, self.new_value_col])\n",
    "        df_copy.reset_index(inplace=True)\n",
    "        df_copy.rename(columns={'index': self.new_date_col, self.value_col: self.new_value_col}, inplace=True)\n",
    "        return df_copy[[self.new_date_col, self.new_value_col]]\n",
    "\n",
    "class SomthDataIntervalValues(BaseEstimator, TransformerMixin): \n",
    "    def __init__(self, value_col='y'): self.value_col = value_col\n",
    "    def fit(self, df, y=None): return self\n",
    "    def transform(self, df): \n",
    "        df_indexed = df.set_index('ds').copy()\n",
    "        series_to_log = df_indexed[self.value_col].apply(lambda x: x if x > 0 else 1e-5)\n",
    "        df_log = np.log(series_to_log)\n",
    "        ma_log = df_log.rolling(window=7, min_periods=1).mean() \n",
    "        df_diff = (df_log - ma_log)\n",
    "        df_transformed = pd.DataFrame(index=df_diff.index)\n",
    "        df_transformed['y_diff_log'] = df_diff; df_transformed['y_ma_log'] = ma_log\n",
    "        df_transformed.dropna(subset=['y_diff_log'], inplace=True); df_transformed.reset_index(inplace=True)\n",
    "        return df_transformed\n",
    "print(\"Classes de Pipeline definidas.\")\n",
    "\n",
    "calculos_erro_list_notebook = [] \n",
    "def calcula_erro_notebook(predicao, real, modelo_nome):\n",
    "    global calculos_erro_list_notebook\n",
    "    def symetric_mean_absolute_percentage_error(actual, predicted) -> float:\n",
    "        actual, predicted = np.array(actual).flatten(), np.array(predicted).flatten()\n",
    "        mask = ~ (np.isnan(actual) | np.isinf(actual) | np.isnan(predicted) | np.isinf(predicted))\n",
    "        actual, predicted = actual[mask], predicted[mask]\n",
    "        if len(actual) == 0: return np.nan\n",
    "        denominator = (np.abs(predicted) + np.abs(actual)) / 2.0; diff = np.abs(predicted - actual)\n",
    "        smape_values = np.where(denominator == 0, 0.0, diff / denominator)\n",
    "        return round(np.mean(smape_values) * 100, 2)\n",
    "    real_flat = np.array(real).flatten(); predicao_flat = np.array(predicao).flatten()\n",
    "    min_len = min(len(real_flat), len(predicao_flat))\n",
    "    if min_len == 0: retorno = {'modelo': modelo_nome, 'mae': np.nan, 'rmse': np.nan, 'smape %': np.nan}\n",
    "    else:\n",
    "        real_flat, predicao_flat = real_flat[:min_len], predicao_flat[:min_len]\n",
    "        mae = mean_absolute_error(real_flat, predicao_flat); rmse = np.sqrt(mean_squared_error(real_flat, predicao_flat))\n",
    "        smape = symetric_mean_absolute_percentage_error(real_flat, predicao_flat)\n",
    "        retorno = {'modelo': modelo_nome, 'mae': round(mae,3), 'rmse': round(rmse,3), 'smape %': smape}\n",
    "    calculos_erro_list_notebook.append(retorno)\n",
    "    return retorno\n",
    "print(\"Função calcula_erro_notebook definida.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# CÉLULA 3: Carregar Dados e Filtrar para Últimos 10 Anos\n",
    "# ==============================================================================\n",
    "try:\n",
    "    df_brent_total_nb = pd.read_csv(file_path_data_notebook)\n",
    "    if 'Date' in df_brent_total_nb.columns and 'Value' in df_brent_total_nb.columns:\n",
    "        df_brent_total_nb.rename(columns={'Date': 'Data', 'Value': 'Value'}, inplace=True)\n",
    "    elif not ('Data' in df_brent_total_nb.columns and 'Value' in df_brent_total_nb.columns):\n",
    "        if len(df_brent_total_nb.columns) >= 2:\n",
    "             df_brent_total_nb.rename(columns={df_brent_total_nb.columns[0]: 'Data', df_brent_total_nb.columns[1]: 'Value'}, inplace=True)\n",
    "        else: raise ValueError(\"CSV não tem colunas 'Data' e 'Value'.\")\n",
    "    df_brent_total_nb['Data'] = pd.to_datetime(df_brent_total_nb['Data'])\n",
    "    df_brent_total_nb.sort_values('Data', inplace=True, ignore_index=True)\n",
    "except Exception as e:\n",
    "    print(f\"Erro Crítico ao carregar dados originais: {e}\")\n",
    "    raise\n",
    "\n",
    "data_final_historico_nb = df_brent_total_nb['Data'].max()\n",
    "data_inicio_10_anos_nb = data_final_historico_nb - pd.DateOffset(years=10)\n",
    "df_brent_10anos_nb = df_brent_total_nb[df_brent_total_nb['Data'] >= data_inicio_10_anos_nb].copy()\n",
    "print(f\"Dados filtrados para os últimos 10 anos (a partir de {data_inicio_10_anos_nb.strftime('%Y-%m-%d')}). Formato: {df_brent_10anos_nb.shape}\")\n",
    "if df_brent_10anos_nb.empty: raise ValueError(\"DataFrame vazio após filtrar pelos últimos 10 anos.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# CÉLULA 4: Treinamento do Modelo ARIMAX (StatsForecast com Features Exógenas) - CORRIGIDO (v3)\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Treinando Modelo AutoARIMAX (StatsForecast) para salvar (10 anos com features) ---\")\n",
    "pipeline_arima_transform_nb = Pipeline([\n",
    "    ('data_prepator_arima', PrepareData(date_col='Data', value_col='Value')),\n",
    "    ('filler_nan_values_arima', FillNANValues(value_col='Value', new_value_col='y', new_date_col='ds')),\n",
    "    ('smoother_data_interval_arima', SomthDataIntervalValues(value_col='y'))\n",
    "])\n",
    "df_transformed_10a_for_arimax = pipeline_arima_transform_nb.fit_transform(df_brent_10anos_nb.copy())\n",
    "\n",
    "# DataFrame que será passado para StatsForecast.fit()\n",
    "# Deve conter 'unique_id', 'ds', 'y' (série alvo transformada) E as colunas de features exógenas escalonadas\n",
    "df_fit_arimax_with_exog = df_transformed_10a_for_arimax[['ds', 'y_diff_log']].copy()\n",
    "df_fit_arimax_with_exog.rename(columns={'y_diff_log': 'y'}, inplace=True)\n",
    "df_fit_arimax_with_exog['unique_id'] = 'Brent'\n",
    "df_fit_arimax_with_exog['ds'] = pd.to_datetime(df_fit_arimax_with_exog['ds'])\n",
    "\n",
    "if df_fit_arimax_with_exog.empty:\n",
    "    raise ValueError(\"DataFrame alvo para ARIMAX está vazio após transformações.\")\n",
    "\n",
    "# Criar features sazonais EXÓGENAS\n",
    "df_exog_features_temp = df_fit_arimax_with_exog[['ds']].copy() \n",
    "df_exog_features_temp['dia_da_semana'] = df_exog_features_temp['ds'].dt.dayofweek\n",
    "df_exog_features_temp['mes_sin'] = np.sin(2 * np.pi * df_exog_features_temp['ds'].dt.month / 12)\n",
    "df_exog_features_temp['mes_cos'] = np.cos(2 * np.pi * df_exog_features_temp['ds'].dt.month / 12)\n",
    "df_exog_features_temp['dia_do_ano_sin'] = np.sin(2 * np.pi * df_exog_features_temp['ds'].dt.dayofyear / 365.25)\n",
    "df_exog_features_temp['dia_do_ano_cos'] = np.cos(2 * np.pi * df_exog_features_temp['ds'].dt.dayofyear / 365.25)\n",
    "X_exog_arimax_unscaled_cols = ['dia_da_semana', 'mes_sin', 'mes_cos', 'dia_do_ano_sin', 'dia_do_ano_cos']\n",
    "X_exog_arimax_unscaled_values = df_exog_features_temp[X_exog_arimax_unscaled_cols]\n",
    "\n",
    "# Escalar features exógenas\n",
    "scaler_exog_arimax_notebook = MinMaxScaler(feature_range=(0,1))\n",
    "X_exog_arimax_scaled_values = scaler_exog_arimax_notebook.fit_transform(X_exog_arimax_unscaled_values)\n",
    "\n",
    "# Adicionar features exógenas escalonadas ao df_fit_arimax_with_exog\n",
    "# Os NOMES DAS COLUNAS DEVEM SER OS MESMOS que X_exog_arimax_unscaled_cols\n",
    "for i, col_name in enumerate(X_exog_arimax_unscaled_cols):\n",
    "    df_fit_arimax_with_exog[col_name] = X_exog_arimax_scaled_values[:, i]\n",
    "\n",
    "# Salvar o scaler das features exógenas do ARIMAX\n",
    "scaler_exog_arimax_path = os.path.join(output_directory, 'scaler_exog_arima.pkl')\n",
    "joblib.dump(scaler_exog_arimax_notebook, scaler_exog_arimax_path)\n",
    "print(f\">>> Scaler para features exógenas do ARIMAX salvo como '{scaler_exog_arimax_path}' <<<\")\n",
    "\n",
    "# Treinar AutoARIMA com features exógenas\n",
    "ARIMAX_SEASON_LENGTH = 0 \n",
    "# === CORREÇÃO FINAL PARA STATSFORECAST >= 2.0.0 ===\n",
    "# O AutoARIMA NÃO recebe 'exogenous' no construtor. Ele detecta automaticamente\n",
    "# colunas extras no DataFrame passado ao fit do StatsForecast.\n",
    "model_sf_arimax_to_save = StatsForecast(\n",
    "    models=[AutoARIMA(season_length=ARIMAX_SEASON_LENGTH)], # SEM 'exogenous' aqui\n",
    "    freq='D',\n",
    "    n_jobs=-1\n",
    ")\n",
    "print(f\"Ajustando AutoARIMAX com season_length={ARIMAX_SEASON_LENGTH}. As features exógenas estão no DataFrame de entrada.\")\n",
    "# O método fit da StatsForecast recebe o DataFrame que já contém 'y' e as colunas exógenas.\n",
    "# O argumento X_df NÃO é usado no StatsForecast.fit()\n",
    "model_sf_arimax_to_save.fit(df_fit_arimax_with_exog) \n",
    "# ============================================================\n",
    "print(\"AutoARIMAX (10 anos com features) ajustado.\")\n",
    "\n",
    "model_sf_arimax_path_to_save = os.path.join(output_directory, 'sarima_model_sf.pkl')\n",
    "model_sf_arimax_to_save.save(model_sf_arimax_path_to_save)\n",
    "print(f\">>> Modelo AutoARIMAX (StatsForecast, 10 anos com features) salvo como '{model_sf_arimax_path_to_save}' <<<\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# CÉLULA 5: Geração de Features ARIMAX para o LSTM Híbrido - CORRIGIDA\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Gerando Features ARIMAX para LSTM Híbrido (baseado no modelo de 10 anos) ---\")\n",
    "h_for_arimax_features_insample = len(df_fit_arimax_with_exog) \n",
    "\n",
    "# Para prever com o modelo ARIMAX treinado, precisamos fornecer as features exógenas\n",
    "# para o período da previsão. Neste caso, são as features históricas.\n",
    "# O X_df para o predict do StatsForecast deve conter 'unique_id', 'ds' e as colunas exógenas.\n",
    "X_df_for_predict_sf = df_fit_arimax_with_exog[['unique_id', 'ds'] + X_exog_arimax_unscaled_cols].copy()\n",
    "\n",
    "predicted_arimax_transformed_features = model_sf_arimax_to_save.predict(\n",
    "    h=h_for_arimax_features_insample, \n",
    "    X_df=X_df_for_predict_sf # Passa o DataFrame com as features exógenas históricas para o predict\n",
    ")\n",
    "\n",
    "# Reconstruir estas features ARIMAX para a escala original\n",
    "ma_log_series_aligned = df_transformed_10a_for_arimax.set_index('ds').loc[df_fit_arimax_with_exog['ds']]['y_ma_log']\n",
    "ma_log_relevant_arimax = ma_log_series_aligned.values\n",
    "\n",
    "len_preds_arimax = 0\n",
    "pred_col_name_arimax = 'AutoARIMA' \n",
    "if pred_col_name_arimax in predicted_arimax_transformed_features.columns:\n",
    "    len_preds_arimax = len(predicted_arimax_transformed_features[pred_col_name_arimax])\n",
    "    arimax_preds_values = predicted_arimax_transformed_features[pred_col_name_arimax].values\n",
    "elif not predicted_arimax_transformed_features.empty:\n",
    "    potential_pred_cols = [col for col in predicted_arimax_transformed_features.columns if col not in ['ds', 'unique_id']]\n",
    "    if potential_pred_cols:\n",
    "        pred_col_name_arimax = potential_pred_cols[0]\n",
    "        arimax_preds_values = predicted_arimax_transformed_features[pred_col_name_arimax].values\n",
    "        len_preds_arimax = len(arimax_preds_values)\n",
    "        print(f\"AVISO: Coluna 'AutoARIMA' não encontrada na previsão ARIMAX. Usando '{pred_col_name_arimax}'.\")\n",
    "    else: raise ValueError(\"Nenhuma coluna de previsão encontrada na saída do AutoARIMAX.\")\n",
    "else: raise ValueError(\"Previsão do AutoARIMAX resultou em DataFrame vazio.\")\n",
    "\n",
    "if len(ma_log_relevant_arimax) > len_preds_arimax:\n",
    "    ma_log_relevant_arimax = ma_log_relevant_arimax[-len_preds_arimax:]\n",
    "elif len(ma_log_relevant_arimax) < len_preds_arimax:\n",
    "    ma_log_relevant_arimax = np.pad(ma_log_relevant_arimax, \n",
    "                                   (0, len_preds_arimax - len(ma_log_relevant_arimax)),\n",
    "                                   'edge')\n",
    "features_arimax_orig_scale_10a = np.exp(\n",
    "    arimax_preds_values + ma_log_relevant_arimax\n",
    ")\n",
    "df_features_arimax_10a = pd.DataFrame({\n",
    "    'ds': predicted_arimax_transformed_features['ds'], \n",
    "    'y_arimax_feature': features_arimax_orig_scale_10a\n",
    "})\n",
    "print(f\"Features ARIMAX (10 anos) geradas e revertidas para escala original. Formato: {df_features_arimax_10a.shape}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# CÉLULA 6: Preparação dos Dados Combinados e Engenharia de Features Sazonais para LSTM\n",
    "# (Esta célula e as seguintes para LSTM permanecem as mesmas da resposta anterior)\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Preparando Dados Combinados e Features Sazonais para LSTM Híbrido (10 anos) ---\")\n",
    "pipeline_normalize_data_nb_10a = Pipeline([\n",
    "    ('data_prepator', PrepareData(date_col='Data', value_col='Value')),\n",
    "    ('filler_nan_values', FillNANValues(value_col='Value', new_value_col='y', new_date_col='ds'))])\n",
    "df_normalized_hist_10a_for_lstm = pipeline_normalize_data_nb_10a.fit_transform(df_brent_10anos_nb.copy())\n",
    "df_normalized_hist_10a_for_lstm['ds'] = pd.to_datetime(df_normalized_hist_10a_for_lstm['ds'])\n",
    "\n",
    "df_combinado_para_lstm_pre_feat = df_normalized_hist_10a_for_lstm.merge(\n",
    "    df_features_arimax_10a.rename(columns={'y_arimax_feature': 'y_arima_feature'}), on='ds', how='left'\n",
    ")\n",
    "df_combinado_para_lstm_pre_feat['y_final_lstm_input'] = df_combinado_para_lstm_pre_feat['y_arima_feature'].fillna(df_combinado_para_lstm_pre_feat['y'])\n",
    "df_combinado_para_lstm_final = df_combinado_para_lstm_pre_feat[['ds', 'y_final_lstm_input']].rename(columns={'y_final_lstm_input': 'y'})\n",
    "\n",
    "df_combinado_para_lstm_final['dia_da_semana'] = df_combinado_para_lstm_final['ds'].dt.dayofweek\n",
    "df_combinado_para_lstm_final['mes_sin'] = np.sin(2 * np.pi * df_combinado_para_lstm_final['ds'].dt.month / 12)\n",
    "df_combinado_para_lstm_final['mes_cos'] = np.cos(2 * np.pi * df_combinado_para_lstm_final['ds'].dt.month / 12)\n",
    "df_combinado_para_lstm_final['dia_do_ano_sin'] = np.sin(2 * np.pi * df_combinado_para_lstm_final['ds'].dt.dayofyear / 365.25)\n",
    "df_combinado_para_lstm_final['dia_do_ano_cos'] = np.cos(2 * np.pi * df_combinado_para_lstm_final['ds'].dt.dayofyear / 365.25)\n",
    "\n",
    "features_lstm_cols = ['y', 'dia_da_semana', 'mes_sin', 'mes_cos', 'dia_do_ano_sin', 'dia_do_ano_cos']\n",
    "df_para_scaler_lstm = df_combinado_para_lstm_final[features_lstm_cols].copy()\n",
    "df_para_scaler_lstm.dropna(inplace=True) \n",
    "if df_para_scaler_lstm.empty: raise ValueError(\"DataFrame para scaler LSTM vazio.\")\n",
    "print(f\"Dados combinados com features sazonais para LSTM. Formato: {df_para_scaler_lstm.shape}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# CÉLULA 7: Normalização, Criação de Sequências e Treinamento do LSTM Híbrido\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Treinamento do Modelo LSTM Híbrido (10 anos com Features Sazonais) ---\")\n",
    "scaler_lstm_hibrido_10a_tuned = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data_hibrido_lstm_10a_tuned = scaler_lstm_hibrido_10a_tuned.fit_transform(df_para_scaler_lstm) \n",
    "scaler_lstm_path_10a_tuned = os.path.join(output_directory, 'scaler.pkl')\n",
    "joblib.dump(scaler_lstm_hibrido_10a_tuned, scaler_lstm_path_10a_tuned)\n",
    "print(f\">>> Scaler LSTM (Híbrido com Features, 10 anos) salvo como '{scaler_lstm_path_10a_tuned}' <<<\")\n",
    "\n",
    "seq_length_lstm_10a_tuned = 90 \n",
    "num_features_lstm = scaled_data_hibrido_lstm_10a_tuned.shape[1]\n",
    "print(f\"LSTM usará seq_length={seq_length_lstm_10a_tuned} e num_features={num_features_lstm}\")\n",
    "\n",
    "def create_sequences_multivariate_notebook(data, seq_length):\n",
    "    xs, ys = [], []\n",
    "    if len(data) <= seq_length: return np.array(xs), np.array(ys)\n",
    "    for i in range(len(data) - seq_length):\n",
    "        xs.append(data[i:(i + seq_length), :]) \n",
    "        ys.append(data[i + seq_length, 0])    \n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "X_hibrido_lstm_10a_tuned, y_hibrido_lstm_10a_tuned = create_sequences_multivariate_notebook(\n",
    "    scaled_data_hibrido_lstm_10a_tuned, seq_length_lstm_10a_tuned)\n",
    "if X_hibrido_lstm_10a_tuned.shape[0] == 0: raise ValueError(\"Nenhuma sequência LSTM multivariada criada.\")\n",
    "\n",
    "tam_validacao_lstm_10a = int(len(X_hibrido_lstm_10a_tuned) * 0.2) \n",
    "if tam_validacao_lstm_10a < 1 and len(X_hibrido_lstm_10a_tuned) > 1: tam_validacao_lstm_10a = 1\n",
    "elif len(X_hibrido_lstm_10a_tuned) <=1 : tam_validacao_lstm_10a = 0\n",
    "if len(X_hibrido_lstm_10a_tuned) - tam_validacao_lstm_10a <= 0 and len(X_hibrido_lstm_10a_tuned) > 0 :\n",
    "    tam_validacao_lstm_10a = 0 \n",
    "    print(\"AVISO: Poucos dados para validação LSTM, treinando com todos os dados de sequência.\")\n",
    "\n",
    "X_train_lstm_10a_tuned, X_val_lstm_10a_tuned = X_hibrido_lstm_10a_tuned[:-tam_validacao_lstm_10a] if tam_validacao_lstm_10a > 0 else X_hibrido_lstm_10a_tuned, \\\n",
    "                                             X_hibrido_lstm_10a_tuned[-tam_validacao_lstm_10a:] if tam_validacao_lstm_10a > 0 else np.array([])\n",
    "y_train_lstm_10a_tuned, y_val_lstm_10a_tuned = y_hibrido_lstm_10a_tuned[:-tam_validacao_lstm_10a] if tam_validacao_lstm_10a > 0 else y_hibrido_lstm_10a_tuned, \\\n",
    "                                             y_hibrido_lstm_10a_tuned[-tam_validacao_lstm_10a:] if tam_validacao_lstm_10a > 0 else np.array([])\n",
    "print(f\"Formato de X_train_lstm_tuned (multivariado): {X_train_lstm_10a_tuned.shape}\")\n",
    "if X_train_lstm_10a_tuned.shape[0] == 0: raise ValueError(\"X_train_lstm_10a_tuned está vazio.\")\n",
    "\n",
    "model_lstm_hibrido_10a_tuned = Sequential()\n",
    "model_lstm_hibrido_10a_tuned.add(LSTM(units=128, return_sequences=True, input_shape=(seq_length_lstm_10a_tuned, num_features_lstm)))\n",
    "model_lstm_hibrido_10a_tuned.add(Dropout(0.3))\n",
    "model_lstm_hibrido_10a_tuned.add(LSTM(units=128, return_sequences=False)) \n",
    "model_lstm_hibrido_10a_tuned.add(Dropout(0.3))\n",
    "model_lstm_hibrido_10a_tuned.add(Dense(64, activation='relu')) \n",
    "model_lstm_hibrido_10a_tuned.add(Dense(1))\n",
    "optimizer_10a_tuned = Adam(learning_rate=0.0005) \n",
    "model_lstm_hibrido_10a_tuned.compile(optimizer=optimizer_10a_tuned, loss='mean_squared_error')\n",
    "\n",
    "print(\"\\nIniciando treinamento do modelo LSTM híbrido com features sazonais (10 anos)...\")\n",
    "epochs_10a_tuned = 200 \n",
    "batch_size_10a_tuned = 64 \n",
    "early_stopping_10a = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=1)\n",
    "validation_data_lstm_10a = (X_val_lstm_10a_tuned, y_val_lstm_10a_tuned) if X_val_lstm_10a_tuned.shape[0] > 0 else None\n",
    "history_lstm_10a_tuned = model_lstm_hibrido_10a_tuned.fit(\n",
    "    X_train_lstm_10a_tuned, y_train_lstm_10a_tuned, epochs=epochs_10a_tuned, batch_size=batch_size_10a_tuned, \n",
    "    validation_data=validation_data_lstm_10a, callbacks=[early_stopping_10a] if validation_data_lstm_10a else [], verbose=1)\n",
    "print(\"Treinamento LSTM (10 anos com features) concluído.\")\n",
    "\n",
    "model_lstm_h5_path_10a_tuned = os.path.join(output_directory, 'lstm_model.h5')\n",
    "model_lstm_hibrido_10a_tuned.save(model_lstm_h5_path_10a_tuned)\n",
    "print(f\">>> Modelo LSTM híbrido (10 anos com features) salvo como '{model_lstm_h5_path_10a_tuned}' <<<\")\n",
    "\n",
    "if validation_data_lstm_10a and 'val_loss' in history_lstm_10a_tuned.history:\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(history_lstm_10a_tuned.history['loss'], label='Perda de Treino')\n",
    "    if 'val_loss' in history_lstm_10a_tuned.history:\n",
    "        plt.plot(history_lstm_10a_tuned.history['val_loss'], label='Perda de Validação')\n",
    "    plt.title('Perda do Modelo LSTM Ajustado (10 anos com Features)')\n",
    "    plt.xlabel('Época'); plt.ylabel('Perda'); plt.legend(); plt.show()\n",
    "\n",
    "# ==============================================================================\n",
    "# NOVA CÉLULA (ou início da CÉLULA 8): Preparação de Dados para Avaliação de Backtesting\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Preparando Dados de Treino e Validação para Avaliação de Backtesting ---\")\n",
    "\n",
    "# Definir proporção do conjunto de validação\n",
    "TAM_VALIDACAO_PROP = 0.2 # 20% para validação\n",
    "\n",
    "# --- Dados Base ---\n",
    "# df_brent_10anos_nb já foi carregado e filtrado na CÉLULA 3\n",
    "\n",
    "# --- Dados para ARIMAX (StatsForecast) e SARIMAX (statsmodels) ---\n",
    "# Pipeline para obter 'ds', 'y' (original após ffill/bfill) e 'y_ma_log', 'y_diff_log' (para ARIMAX)\n",
    "pipeline_preprocess_eval = Pipeline([\n",
    "    ('data_preparator', PrepareData(date_col='Data', value_col='Value')),\n",
    "    ('filler_nan_values', FillNANValues(value_col='Value', new_value_col='y', new_date_col='ds')),\n",
    "    # SmootherDataIntervalValues é específico para a transformação do ARIMAX (StatsForecast)\n",
    "    # Para SARIMAX (statsmodels), aplicaremos log separadamente.\n",
    "])\n",
    "\n",
    "# 1. Dados para ARIMAX (StatsForecast)\n",
    "df_processed_for_arimax_eval = pipeline_preprocess_eval.fit_transform(df_brent_10anos_nb.copy())\n",
    "# Aplicar o smoother que gera y_diff_log e y_ma_log\n",
    "smoother_arimax_eval = SomthDataIntervalValues(value_col='y')\n",
    "df_transformed_arimax_eval = smoother_arimax_eval.fit_transform(df_processed_for_arimax_eval.copy())\n",
    "df_transformed_arimax_eval['ds'] = pd.to_datetime(df_transformed_arimax_eval['ds'])\n",
    "\n",
    "# Criar features exógenas para ARIMAX (usando a lógica da CÉLULA 4)\n",
    "df_exog_features_arimax_eval_temp = df_transformed_arimax_eval[['ds']].copy()\n",
    "df_exog_features_arimax_eval_temp['dia_da_semana'] = df_exog_features_arimax_eval_temp['ds'].dt.dayofweek\n",
    "df_exog_features_arimax_eval_temp['mes_sin'] = np.sin(2 * np.pi * df_exog_features_arimax_eval_temp['ds'].dt.month / 12)\n",
    "df_exog_features_arimax_eval_temp['mes_cos'] = np.cos(2 * np.pi * df_exog_features_arimax_eval_temp['ds'].dt.month / 12)\n",
    "df_exog_features_arimax_eval_temp['dia_do_ano_sin'] = np.sin(2 * np.pi * df_exog_features_arimax_eval_temp['ds'].dt.dayofyear / 365.25)\n",
    "df_exog_features_arimax_eval_temp['dia_do_ano_cos'] = np.cos(2 * np.pi * df_exog_features_arimax_eval_temp['ds'].dt.dayofyear / 365.25)\n",
    "# X_exog_arimax_unscaled_cols já deve estar definido globalmente pela CÉLULA 4, mas podemos redefinir para clareza\n",
    "X_exog_cols_eval = ['dia_da_semana', 'mes_sin', 'mes_cos', 'dia_do_ano_sin', 'dia_do_ano_cos']\n",
    "X_exog_arimax_unscaled_eval = df_exog_features_arimax_eval_temp[X_exog_cols_eval]\n",
    "\n",
    "# Alinhar dados para divisão\n",
    "df_target_arimax_eval = df_transformed_arimax_eval.set_index('ds')['y_diff_log']\n",
    "df_ma_log_arimax_eval = df_transformed_arimax_eval.set_index('ds')['y_ma_log']\n",
    "df_y_real_arimax_eval = df_processed_for_arimax_eval.set_index('ds')['y'] # y original para comparação\n",
    "\n",
    "common_idx_arimax = df_target_arimax_eval.index.intersection(X_exog_arimax_unscaled_eval.index).intersection(df_ma_log_arimax_eval.index).intersection(df_y_real_arimax_eval.index)\n",
    "df_target_arimax_eval = df_target_arimax_eval.loc[common_idx_arimax]\n",
    "X_exog_arimax_unscaled_eval = X_exog_arimax_unscaled_eval.loc[common_idx_arimax]\n",
    "df_ma_log_arimax_eval = df_ma_log_arimax_eval.loc[common_idx_arimax]\n",
    "df_y_real_arimax_eval = df_y_real_arimax_eval.loc[common_idx_arimax]\n",
    "\n",
    "\n",
    "n_total_arimax_eval = len(df_target_arimax_eval)\n",
    "n_val_arimax_eval = int(n_total_arimax_eval * TAM_VALIDACAO_PROP)\n",
    "n_train_arimax_eval = n_total_arimax_eval - n_val_arimax_eval\n",
    "\n",
    "# Divisão ARIMAX\n",
    "y_train_arimax_target = df_target_arimax_eval.iloc[:n_train_arimax_eval]\n",
    "X_train_arimax_exog_unscaled = X_exog_arimax_unscaled_eval.iloc[:n_train_arimax_eval]\n",
    "X_val_arimax_exog_unscaled = X_exog_arimax_unscaled_eval.iloc[n_train_arimax_eval:]\n",
    "y_val_arimax_ma_log = df_ma_log_arimax_eval.iloc[n_train_arimax_eval:].values\n",
    "y_val_arimax_real = df_y_real_arimax_eval.iloc[n_train_arimax_eval:].values\n",
    "ds_val_arimax = df_target_arimax_eval.index[n_train_arimax_eval:]\n",
    "\n",
    "\n",
    "# Escalar exógenas do ARIMAX (StatsForecast) para avaliação (treinar scaler SÓ no treino)\n",
    "scaler_exog_arimax_eval = MinMaxScaler(feature_range=(0,1))\n",
    "X_train_arimax_exog_scaled = scaler_exog_arimax_eval.fit_transform(X_train_arimax_exog_unscaled)\n",
    "X_val_arimax_exog_scaled = scaler_exog_arimax_eval.transform(X_val_arimax_exog_unscaled)\n",
    "\n",
    "df_treino_arimax_statsforecast_eval = pd.DataFrame({\n",
    "    'ds': y_train_arimax_target.index,\n",
    "    'y': y_train_arimax_target.values,\n",
    "    'unique_id': 'Brent_eval_arimax'\n",
    "})\n",
    "for i, col in enumerate(X_exog_cols_eval):\n",
    "    df_treino_arimax_statsforecast_eval[col] = X_train_arimax_exog_scaled[:, i]\n",
    "\n",
    "df_validacao_arimax_statsforecast_X_eval = pd.DataFrame(X_val_arimax_exog_scaled, columns=X_exog_cols_eval)\n",
    "df_validacao_arimax_statsforecast_X_eval['ds'] = ds_val_arimax\n",
    "df_validacao_arimax_statsforecast_X_eval['unique_id'] = 'Brent_eval_arimax'\n",
    "\n",
    "\n",
    "print(f\"ARIMAX (StatsForecast) Eval - Treino: {len(df_treino_arimax_statsforecast_eval)}, Validação: {len(df_validacao_arimax_statsforecast_X_eval)}\")\n",
    "\n",
    "# 2. Dados para SARIMAX (statsmodels)\n",
    "# Usaremos df_processed_for_arimax_eval que tem 'ds' e 'y' (original preenchida)\n",
    "df_sarimax_sm_base_eval = df_processed_for_arimax_eval.copy()\n",
    "df_sarimax_sm_base_eval['y_log'] = np.log(df_sarimax_sm_base_eval['y'].apply(lambda x: x if x > 0 else 1e-5))\n",
    "\n",
    "# Features exógenas (mesmas do ARIMAX para simplificar)\n",
    "df_exog_features_sarimax_sm_eval_temp = df_sarimax_sm_base_eval[['ds']].copy()\n",
    "df_exog_features_sarimax_sm_eval_temp['dia_da_semana'] = df_exog_features_sarimax_sm_eval_temp['ds'].dt.dayofweek\n",
    "df_exog_features_sarimax_sm_eval_temp['mes_sin'] = np.sin(2 * np.pi * df_exog_features_sarimax_sm_eval_temp['ds'].dt.month / 12)\n",
    "df_exog_features_sarimax_sm_eval_temp['mes_cos'] = np.cos(2 * np.pi * df_exog_features_sarimax_sm_eval_temp['ds'].dt.month / 12)\n",
    "df_exog_features_sarimax_sm_eval_temp['dia_do_ano_sin'] = np.sin(2 * np.pi * df_exog_features_sarimax_sm_eval_temp['ds'].dt.dayofyear / 365.25)\n",
    "df_exog_features_sarimax_sm_eval_temp['dia_do_ano_cos'] = np.cos(2 * np.pi * df_exog_features_sarimax_sm_eval_temp['ds'].dt.dayofyear / 365.25)\n",
    "X_exog_sarimax_sm_unscaled_eval = df_exog_features_sarimax_sm_eval_temp[X_exog_cols_eval]\n",
    "\n",
    "# Alinhar\n",
    "df_target_sarimax_sm_eval = df_sarimax_sm_base_eval.set_index('ds')['y_log']\n",
    "df_y_real_sarimax_sm_eval = df_sarimax_sm_base_eval.set_index('ds')['y']\n",
    "\n",
    "common_idx_sarimax_sm = df_target_sarimax_sm_eval.index.intersection(X_exog_sarimax_sm_unscaled_eval.index).intersection(df_y_real_sarimax_sm_eval.index)\n",
    "df_target_sarimax_sm_eval = df_target_sarimax_sm_eval.loc[common_idx_sarimax_sm]\n",
    "X_exog_sarimax_sm_unscaled_eval = X_exog_sarimax_sm_unscaled_eval.loc[common_idx_sarimax_sm]\n",
    "df_y_real_sarimax_sm_eval = df_y_real_sarimax_sm_eval.loc[common_idx_sarimax_sm]\n",
    "\n",
    "n_total_sarimax_sm_eval = len(df_target_sarimax_sm_eval)\n",
    "n_val_sarimax_sm_eval = int(n_total_sarimax_sm_eval * TAM_VALIDACAO_PROP)\n",
    "n_train_sarimax_sm_eval = n_total_sarimax_sm_eval - n_val_sarimax_sm_eval\n",
    "\n",
    "# Divisão SARIMAX (statsmodels)\n",
    "y_train_sarimax_sm_log = df_target_sarimax_sm_eval.iloc[:n_train_sarimax_sm_eval]\n",
    "X_train_sarimax_sm_exog_unscaled = X_exog_sarimax_sm_unscaled_eval.iloc[:n_train_sarimax_sm_eval]\n",
    "X_val_sarimax_sm_exog_unscaled = X_exog_sarimax_sm_unscaled_eval.iloc[n_train_sarimax_sm_eval:]\n",
    "y_val_sarimax_sm_real = df_y_real_sarimax_sm_eval.iloc[n_train_sarimax_sm_eval:].values\n",
    "\n",
    "# Escalar exógenas do SARIMAX (statsmodels) para avaliação\n",
    "scaler_exog_sarimax_sm_eval = MinMaxScaler(feature_range=(0,1))\n",
    "X_train_sarimax_sm_exog_scaled = scaler_exog_sarimax_sm_eval.fit_transform(X_train_sarimax_sm_exog_unscaled)\n",
    "X_val_sarimax_sm_exog_scaled = scaler_exog_sarimax_sm_eval.transform(X_val_sarimax_sm_exog_unscaled)\n",
    "\n",
    "print(f\"SARIMAX (statsmodels) Eval - Treino: {len(y_train_sarimax_sm_log)}, Validação: {len(X_val_sarimax_sm_exog_scaled)}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# CÉLULA 9: Avaliação dos Modelos (Backtesting) e Salvamento de df_erros.csv\n",
    "# ==============================================================================\n",
    "calculos_erro_list_notebook = []\n",
    "print(f\"Iniciando Avaliação. Tamanho inicial de calculos_erro_list_notebook: {len(calculos_erro_list_notebook)}\")\n",
    "\n",
    "# --- 1. Avaliação do Modelo AutoARIMAX (StatsForecast) ---\n",
    "print(\"\\n--- Avaliando Modelo AutoARIMAX (StatsForecast) no conjunto de validação ---\")\n",
    "# Dados: df_treino_arimax_statsforecast_eval, df_validacao_arimax_statsforecast_X_eval\n",
    "# y_val_arimax_ma_log, y_val_arimax_real\n",
    "# ARIMAX_SEASON_LENGTH (definido na CÉLULA 4)\n",
    "\n",
    "if len(df_treino_arimax_statsforecast_eval) > 0 and len(df_validacao_arimax_statsforecast_X_eval) > 0:\n",
    "    try:\n",
    "        model_sf_arimax_eval = StatsForecast(\n",
    "            models=[AutoARIMA(season_length=ARIMAX_SEASON_LENGTH)],\n",
    "            freq='D',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        print(f\"Ajustando AutoARIMAX de avaliação (StatsForecast) com {len(df_treino_arimax_statsforecast_eval)} pontos de treino...\")\n",
    "        model_sf_arimax_eval.fit(df_treino_arimax_statsforecast_eval) # df_treino_arimax_statsforecast_eval já tem 'y' e exógenas escaladas\n",
    "        print(\"AutoARIMAX de avaliação (StatsForecast) ajustado.\")\n",
    "\n",
    "        h_val_arimax = len(df_validacao_arimax_statsforecast_X_eval)\n",
    "        pred_val_arimax_diff_log_sf = model_sf_arimax_eval.predict(\n",
    "            h=h_val_arimax,\n",
    "            X_df=df_validacao_arimax_statsforecast_X_eval\n",
    "        )\n",
    "\n",
    "        pred_col_arimax_eval_sf = 'AutoARIMA'\n",
    "        if pred_col_arimax_eval_sf not in pred_val_arimax_diff_log_sf.columns and not pred_val_arimax_diff_log_sf.empty:\n",
    "            potential_cols = [col for col in pred_val_arimax_diff_log_sf.columns if col not in ['ds', 'unique_id']]\n",
    "            if potential_cols: pred_col_arimax_eval_sf = potential_cols[0]\n",
    "            else: raise ValueError(\"Coluna de previsão não encontrada na validação ARIMAX (StatsForecast)\")\n",
    "        \n",
    "        pred_values_arimax_diff_log_eval = pred_val_arimax_diff_log_sf[pred_col_arimax_eval_sf].values\n",
    "\n",
    "        if len(pred_values_arimax_diff_log_eval) == len(y_val_arimax_ma_log):\n",
    "            pred_val_arimax_log_scale_eval = pred_values_arimax_diff_log_eval + y_val_arimax_ma_log\n",
    "            pred_val_arimax_original_scale_eval = np.exp(pred_val_arimax_log_scale_eval)\n",
    "\n",
    "            if len(pred_val_arimax_original_scale_eval) == len(y_val_arimax_real):\n",
    "                print(\"\\nMétricas para AutoARIMAX (StatsForecast) - Avaliação:\")\n",
    "                metricas_arimax_sf = calcula_erro_notebook(\n",
    "                    pred_val_arimax_original_scale_eval,\n",
    "                    y_val_arimax_real,\n",
    "                    \"ARIMAX_StatsForecast_Val\"\n",
    "                )\n",
    "                print(metricas_arimax_sf)\n",
    "            else:\n",
    "                print(f\"AVISO ARIMAX (SF) Eval: Discrepância de tamanho ({len(pred_val_arimax_original_scale_eval)}) vs ({len(y_val_arimax_real)}).\")\n",
    "        else:\n",
    "            print(f\"AVISO ARIMAX (SF) Eval: Discrepância de tamanho diff_log ({len(pred_values_arimax_diff_log_eval)}) vs ma_log ({len(y_val_arimax_ma_log)}).\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERRO durante avaliação do AutoARIMAX (StatsForecast): {e}\")\n",
    "        calcula_erro_notebook(np.array([]), np.array([]), \"ARIMAX_StatsForecast_Val_Falha\")\n",
    "else:\n",
    "    print(\"AVISO: Dados insuficientes para avaliação do ARIMAX (StatsForecast).\")\n",
    "\n",
    "\n",
    "# --- 2. Avaliação do Modelo SARIMAX (statsmodels) ---\n",
    "print(\"\\n--- Avaliando Modelo SARIMAX (statsmodels) no conjunto de validação ---\")\n",
    "# Dados: y_train_sarimax_sm_log, X_train_sarimax_sm_exog_scaled,\n",
    "# X_val_sarimax_sm_exog_scaled, y_val_sarimax_sm_real\n",
    "\n",
    "if len(y_train_sarimax_sm_log) > 0 and len(X_val_sarimax_sm_exog_scaled) > 0:\n",
    "    try:\n",
    "        adf_result_sm = adfuller(y_train_sarimax_sm_log.dropna())\n",
    "        d_order_sarimax_sm_eval = 1 if adf_result_sm[1] > 0.05 else 0\n",
    "        print(f\"SARIMAX (statsmodels) Eval - Ordem d estimada: {d_order_sarimax_sm_eval} (p-value ADF: {adf_result_sm[1]:.3f})\")\n",
    "\n",
    "        model_sarimax_sm_eval = SARIMAX(\n",
    "            endog=y_train_sarimax_sm_log,\n",
    "            exog=X_train_sarimax_sm_exog_scaled,\n",
    "            order=(2, d_order_sarimax_sm_eval, 2), # Exemplo de ordem não-sazonal\n",
    "            seasonal_order=(1, 1, 1, 7),       # Exemplo de ordem sazonal (semanal)\n",
    "            enforce_stationarity=False,\n",
    "            enforce_invertibility=False,\n",
    "            initialization='approximate_diffuse'\n",
    "        )\n",
    "        print(f\"Ajustando SARIMAX (statsmodels) de avaliação com {len(y_train_sarimax_sm_log)} pontos de treino...\")\n",
    "        results_sarimax_sm_eval = model_sarimax_sm_eval.fit(disp=False)\n",
    "        print(\"SARIMAX (statsmodels) de avaliação ajustado.\")\n",
    "\n",
    "        h_val_sarimax_sm = len(X_val_sarimax_sm_exog_scaled)\n",
    "        pred_val_sarimax_log_scale_sm = results_sarimax_sm_eval.forecast(\n",
    "            steps=h_val_sarimax_sm,\n",
    "            exog=X_val_sarimax_sm_exog_scaled\n",
    "        )\n",
    "        pred_val_sarimax_original_scale_sm = np.exp(pred_val_sarimax_log_scale_sm.values)\n",
    "\n",
    "        if len(pred_val_sarimax_original_scale_sm) == len(y_val_sarimax_sm_real):\n",
    "            print(\"\\nMétricas para SARIMAX (statsmodels) - Avaliação:\")\n",
    "            metricas_sarimax_sm = calcula_erro_notebook(\n",
    "                pred_val_sarimax_original_scale_sm,\n",
    "                y_val_sarimax_sm_real,\n",
    "                \"SARIMAX_Statsmodels_Val\"\n",
    "            )\n",
    "            print(metricas_sarimax_sm)\n",
    "        else:\n",
    "            print(f\"AVISO SARIMAX (SM) Eval: Discrepância de tamanho ({len(pred_val_sarimax_original_scale_sm)}) vs ({len(y_val_sarimax_sm_real)}).\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERRO durante avaliação do SARIMAX (statsmodels): {e}\")\n",
    "        calcula_erro_notebook(np.array([]), np.array([]), \"SARIMAX_Statsmodels_Val_Falha\")\n",
    "else:\n",
    "    print(\"AVISO: Dados insuficientes para avaliação do SARIMAX (statsmodels).\")\n",
    "\n",
    "\n",
    "# --- 3. Avaliação do Modelo LSTM Híbrido ---\n",
    "# O código original de avaliação do LSTM é mantido aqui.\n",
    "# Certifique-se que as variáveis LSTM (model_lstm_hibrido_10a_tuned, X_val_lstm_10a_tuned, etc.)\n",
    "# da CÉLULA 7 estão disponíveis e corretas.\n",
    "print(f\"\\nAntes da avaliação LSTM. Tamanho de calculos_erro_list_notebook: {len(calculos_erro_list_notebook)}\")\n",
    "if 'validation_data_lstm_10a' in locals() and validation_data_lstm_10a and X_val_lstm_10a_tuned.shape[0] > 0:\n",
    "    try:\n",
    "        pred_val_lstm_scaled_10a = model_lstm_hibrido_10a_tuned.predict(X_val_lstm_10a_tuned)\n",
    "        dummy_array_for_inverse = np.zeros((len(pred_val_lstm_scaled_10a), num_features_lstm)) # num_features_lstm da CÉLULA 7\n",
    "        dummy_array_for_inverse[:, 0] = pred_val_lstm_scaled_10a.flatten()\n",
    "        pred_val_lstm_original_scale_10a = scaler_lstm_hibrido_10a_tuned.inverse_transform(dummy_array_for_inverse)[:, 0] # scaler_lstm_hibrido_10a_tuned da CÉLULA 7\n",
    "\n",
    "        dummy_array_for_inverse_y_true = np.zeros((len(y_val_lstm_10a_tuned), num_features_lstm)) # y_val_lstm_10a_tuned da CÉLULA 7\n",
    "        dummy_array_for_inverse_y_true[:, 0] = y_val_lstm_10a_tuned.flatten()\n",
    "        y_val_original_scale_10a = scaler_lstm_hibrido_10a_tuned.inverse_transform(dummy_array_for_inverse_y_true)[:, 0]\n",
    "\n",
    "        print(\"\\nMétricas para LSTM Híbrido (10 anos com Features) no conjunto de validação:\")\n",
    "        metricas_lstm = calcula_erro_notebook(pred_val_lstm_original_scale_10a, y_val_original_scale_10a, \"LSTM_Hibrido_10Anos_Val\")\n",
    "        print(metricas_lstm)\n",
    "    except Exception as e:\n",
    "        print(f\"ERRO durante avaliação do LSTM: {e}\")\n",
    "        calcula_erro_notebook(np.array([]), np.array([]), \"LSTM_Hibrido_10Anos_Val_Falha\")\n",
    "else:\n",
    "    print(\"AVISO: Dados de validação para LSTM não disponíveis ou insuficientes. Avaliação LSTM pulada.\")\n",
    "\n",
    "\n",
    "# --- Salvamento Final do df_erros.csv ---\n",
    "df_erros_path_notebook = os.path.join(output_directory, 'df_erros.csv')\n",
    "if calculos_erro_list_notebook:\n",
    "    df_erros_final_notebook = pd.DataFrame(calculos_erro_list_notebook)\n",
    "    df_erros_final_notebook.to_csv(df_erros_path_notebook, index=False)\n",
    "    print(f\"\\n>>> Métricas de erro salvas como '{df_erros_path_notebook}' <<<\")\n",
    "    print(\"\\nDataFrame de Erros (do Notebook):\\n\", df_erros_final_notebook)\n",
    "else:\n",
    "    print(f\"AVISO: 'calculos_erro_list_notebook' não populada. '{df_erros_path_notebook}' não foi criado.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
