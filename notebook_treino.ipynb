{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e186f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CÉLULA 1: Imports e Configurações Iniciais\n",
    "# ==============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import joblib\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Imports para Modelagem\n",
    "import statsforecast # Para verificar a versão\n",
    "from statsforecast import StatsForecast\n",
    "from statsforecast.models import AutoARIMA\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from statsmodels.tsa.stattools import adfuller \n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf \n",
    "\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"StatsForecast Version: {statsforecast.__version__}\")\n",
    "\n",
    "# --- Configuração de Caminhos ---\n",
    "output_directory = \"./\" \n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "file_path_data_notebook = 'C:/Users/ReDragon/Desktop/TECH_CHALLENTE_F4/dados/preco_petroleo.csv' \n",
    "print(f\"Usando arquivo de dados: {file_path_data_notebook}\")\n",
    "print(f\"Arquivos de modelo serão salvos em: {os.path.abspath(output_directory)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d385a654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CÉLULA 2: Definição das Classes do Pipeline e Função de Erro\n",
    "# ==============================================================================\n",
    "class PrepareData(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, date_col='Data', value_col='Value'):\n",
    "        self.date_col = date_col\n",
    "        self.value_col = value_col\n",
    "    def fit(self, df, y=None): return self\n",
    "    def transform(self, df):\n",
    "        df_copy = df.copy()\n",
    "        if 'Date' in df_copy.columns and self.date_col == 'Data': df_copy.rename(columns={'Date': 'Data'}, inplace=True)\n",
    "        if 'Preço - petróleo bruto - Brent (FOB)' in df_copy.columns and self.value_col == 'Value': df_copy.rename(columns={'Preço - petróleo bruto - Brent (FOB)': 'Value'}, inplace=True)\n",
    "        if self.date_col not in df_copy.columns: raise ValueError(f\"Coluna de data '{self.date_col}' não encontrada.\")\n",
    "        if self.value_col not in df_copy.columns: raise ValueError(f\"Coluna de valor '{self.value_col}' não encontrada.\")\n",
    "        df_copy[self.date_col] = pd.to_datetime(df_copy[self.date_col])\n",
    "        df_copy.set_index(self.date_col, inplace=True); df_copy[self.value_col] = df_copy[self.value_col].astype(float)\n",
    "        df_copy.dropna(subset=[self.value_col], inplace=True); df_copy.sort_index(inplace=True, ascending=True)\n",
    "        return df_copy\n",
    "\n",
    "class FillNANValues(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, value_col='Value', new_value_col='y', new_date_col='ds'):\n",
    "        self.value_col, self.new_value_col, self.new_date_col = value_col, new_value_col, new_date_col\n",
    "    def fit(self, df, y=None): return self\n",
    "    def transform(self, df):\n",
    "        df_copy = df.copy()\n",
    "        if not isinstance(df_copy.index, pd.DatetimeIndex): raise ValueError(\"DatetimeIndex esperado.\")\n",
    "        start_date, end_date = df_copy.index.min(), df_copy.index.max()\n",
    "        if pd.isna(start_date) or pd.isna(end_date) or df_copy.empty: return pd.DataFrame(columns=[self.new_date_col, self.new_value_col])\n",
    "        df_copy = df_copy.reindex(pd.date_range(start=start_date, end=end_date, freq='D'))\n",
    "        df_copy[self.value_col].ffill(inplace=True); df_copy[self.value_col].bfill(inplace=True)\n",
    "        df_copy.dropna(subset=[self.value_col], inplace=True)\n",
    "        if df_copy.empty: return pd.DataFrame(columns=[self.new_date_col, self.new_value_col])\n",
    "        df_copy.reset_index(inplace=True)\n",
    "        df_copy.rename(columns={'index': self.new_date_col, self.value_col: self.new_value_col}, inplace=True)\n",
    "        return df_copy[[self.new_date_col, self.new_value_col]]\n",
    "\n",
    "class SomthDataIntervalValues(BaseEstimator, TransformerMixin): \n",
    "    def __init__(self, value_col='y'): self.value_col = value_col\n",
    "    def fit(self, df, y=None): return self\n",
    "    def transform(self, df): \n",
    "        df_indexed = df.set_index('ds').copy()\n",
    "        series_to_log = df_indexed[self.value_col].apply(lambda x: x if x > 0 else 1e-5)\n",
    "        df_log = np.log(series_to_log)\n",
    "        ma_log = df_log.rolling(window=7, min_periods=1).mean() \n",
    "        df_diff = (df_log - ma_log)\n",
    "        df_transformed = pd.DataFrame(index=df_diff.index)\n",
    "        df_transformed['y_diff_log'] = df_diff; df_transformed['y_ma_log'] = ma_log\n",
    "        df_transformed.dropna(subset=['y_diff_log'], inplace=True); df_transformed.reset_index(inplace=True)\n",
    "        return df_transformed\n",
    "print(\"Classes de Pipeline definidas.\")\n",
    "\n",
    "calculos_erro_list_notebook = [] \n",
    "def calcula_erro_notebook(predicao, real, modelo_nome):\n",
    "    global calculos_erro_list_notebook\n",
    "    def symetric_mean_absolute_percentage_error(actual, predicted) -> float:\n",
    "        actual, predicted = np.array(actual).flatten(), np.array(predicted).flatten()\n",
    "        mask = ~ (np.isnan(actual) | np.isinf(actual) | np.isnan(predicted) | np.isinf(predicted))\n",
    "        actual, predicted = actual[mask], predicted[mask]\n",
    "        if len(actual) == 0: return np.nan\n",
    "        denominator = (np.abs(predicted) + np.abs(actual)) / 2.0; diff = np.abs(predicted - actual)\n",
    "        smape_values = np.where(denominator == 0, 0.0, diff / denominator)\n",
    "        return round(np.mean(smape_values) * 100, 2)\n",
    "    real_flat = np.array(real).flatten(); predicao_flat = np.array(predicao).flatten()\n",
    "    min_len = min(len(real_flat), len(predicao_flat))\n",
    "    if min_len == 0: retorno = {'modelo': modelo_nome, 'mae': np.nan, 'rmse': np.nan, 'smape %': np.nan}\n",
    "    else:\n",
    "        real_flat, predicao_flat = real_flat[:min_len], predicao_flat[:min_len]\n",
    "        mae = mean_absolute_error(real_flat, predicao_flat); rmse = np.sqrt(mean_squared_error(real_flat, predicao_flat))\n",
    "        smape = symetric_mean_absolute_percentage_error(real_flat, predicao_flat)\n",
    "        retorno = {'modelo': modelo_nome, 'mae': round(mae,3), 'rmse': round(rmse,3), 'smape %': smape}\n",
    "    calculos_erro_list_notebook.append(retorno)\n",
    "    return retorno\n",
    "print(\"Função calcula_erro_notebook definida.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117d0697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CÉLULA 3: Carregar Dados e Filtrar para Últimos 10 Anos\n",
    "# ==============================================================================\n",
    "try:\n",
    "    df_brent_total_nb = pd.read_csv(file_path_data_notebook)\n",
    "    if 'Date' in df_brent_total_nb.columns and 'Value' in df_brent_total_nb.columns:\n",
    "        df_brent_total_nb.rename(columns={'Date': 'Data', 'Value': 'Value'}, inplace=True)\n",
    "    elif not ('Data' in df_brent_total_nb.columns and 'Value' in df_brent_total_nb.columns):\n",
    "        if len(df_brent_total_nb.columns) >= 2:\n",
    "             df_brent_total_nb.rename(columns={df_brent_total_nb.columns[0]: 'Data', df_brent_total_nb.columns[1]: 'Value'}, inplace=True)\n",
    "        else: raise ValueError(\"CSV não tem colunas 'Data' e 'Value'.\")\n",
    "    df_brent_total_nb['Data'] = pd.to_datetime(df_brent_total_nb['Data'])\n",
    "    df_brent_total_nb.sort_values('Data', inplace=True, ignore_index=True)\n",
    "except Exception as e:\n",
    "    print(f\"Erro Crítico ao carregar dados originais: {e}\")\n",
    "    raise\n",
    "\n",
    "data_final_historico_nb = df_brent_total_nb['Data'].max()\n",
    "data_inicio_10_anos_nb = data_final_historico_nb - pd.DateOffset(years=10)\n",
    "df_brent_10anos_nb = df_brent_total_nb[df_brent_total_nb['Data'] >= data_inicio_10_anos_nb].copy()\n",
    "print(f\"Dados filtrados para os últimos 10 anos (a partir de {data_inicio_10_anos_nb.strftime('%Y-%m-%d')}). Formato: {df_brent_10anos_nb.shape}\")\n",
    "if df_brent_10anos_nb.empty: raise ValueError(\"DataFrame vazio após filtrar pelos últimos 10 anos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4a2f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CÉLULA 4: Treinamento do Modelo ARIMAX (StatsForecast com Features Exógenas) - CORRIGIDO (v3)\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Treinando Modelo AutoARIMAX (StatsForecast) para salvar (10 anos com features) ---\")\n",
    "pipeline_arima_transform_nb = Pipeline([\n",
    "    ('data_prepator_arima', PrepareData(date_col='Data', value_col='Value')),\n",
    "    ('filler_nan_values_arima', FillNANValues(value_col='Value', new_value_col='y', new_date_col='ds')),\n",
    "    ('smoother_data_interval_arima', SomthDataIntervalValues(value_col='y'))\n",
    "])\n",
    "df_transformed_10a_for_arimax = pipeline_arima_transform_nb.fit_transform(df_brent_10anos_nb.copy())\n",
    "\n",
    "# DataFrame que será passado para StatsForecast.fit()\n",
    "# Deve conter 'unique_id', 'ds', 'y' (série alvo transformada) E as colunas de features exógenas escalonadas\n",
    "df_fit_arimax_with_exog = df_transformed_10a_for_arimax[['ds', 'y_diff_log']].copy()\n",
    "df_fit_arimax_with_exog.rename(columns={'y_diff_log': 'y'}, inplace=True)\n",
    "df_fit_arimax_with_exog['unique_id'] = 'Brent'\n",
    "df_fit_arimax_with_exog['ds'] = pd.to_datetime(df_fit_arimax_with_exog['ds'])\n",
    "\n",
    "if df_fit_arimax_with_exog.empty:\n",
    "    raise ValueError(\"DataFrame alvo para ARIMAX está vazio após transformações.\")\n",
    "\n",
    "# Criar features sazonais EXÓGENAS\n",
    "df_exog_features_temp = df_fit_arimax_with_exog[['ds']].copy() \n",
    "df_exog_features_temp['dia_da_semana'] = df_exog_features_temp['ds'].dt.dayofweek\n",
    "df_exog_features_temp['mes_sin'] = np.sin(2 * np.pi * df_exog_features_temp['ds'].dt.month / 12)\n",
    "df_exog_features_temp['mes_cos'] = np.cos(2 * np.pi * df_exog_features_temp['ds'].dt.month / 12)\n",
    "df_exog_features_temp['dia_do_ano_sin'] = np.sin(2 * np.pi * df_exog_features_temp['ds'].dt.dayofyear / 365.25)\n",
    "df_exog_features_temp['dia_do_ano_cos'] = np.cos(2 * np.pi * df_exog_features_temp['ds'].dt.dayofyear / 365.25)\n",
    "X_exog_arimax_unscaled_cols = ['dia_da_semana', 'mes_sin', 'mes_cos', 'dia_do_ano_sin', 'dia_do_ano_cos']\n",
    "X_exog_arimax_unscaled_values = df_exog_features_temp[X_exog_arimax_unscaled_cols]\n",
    "\n",
    "# Escalar features exógenas\n",
    "scaler_exog_arimax_notebook = MinMaxScaler(feature_range=(0,1))\n",
    "X_exog_arimax_scaled_values = scaler_exog_arimax_notebook.fit_transform(X_exog_arimax_unscaled_values)\n",
    "\n",
    "# Adicionar features exógenas escalonadas ao df_fit_arimax_with_exog\n",
    "# Os NOMES DAS COLUNAS DEVEM SER OS MESMOS que X_exog_arimax_unscaled_cols\n",
    "for i, col_name in enumerate(X_exog_arimax_unscaled_cols):\n",
    "    df_fit_arimax_with_exog[col_name] = X_exog_arimax_scaled_values[:, i]\n",
    "\n",
    "# Salvar o scaler das features exógenas do ARIMAX\n",
    "scaler_exog_arimax_path = os.path.join(output_directory, 'scaler_exog_arima.pkl')\n",
    "joblib.dump(scaler_exog_arimax_notebook, scaler_exog_arimax_path)\n",
    "print(f\">>> Scaler para features exógenas do ARIMAX salvo como '{scaler_exog_arimax_path}' <<<\")\n",
    "\n",
    "# Treinar AutoARIMA com features exógenas\n",
    "ARIMAX_SEASON_LENGTH = 30 \n",
    "# === CORREÇÃO FINAL PARA STATSFORECAST >= 2.0.0 ===\n",
    "# O AutoARIMA NÃO recebe 'exogenous' no construtor. Ele detecta automaticamente\n",
    "# colunas extras no DataFrame passado ao fit do StatsForecast.\n",
    "model_sf_arimax_to_save = StatsForecast(\n",
    "    models=[AutoARIMA(season_length=ARIMAX_SEASON_LENGTH)], # SEM 'exogenous' aqui\n",
    "    freq='D',\n",
    "    n_jobs=-1\n",
    ")\n",
    "print(f\"Ajustando AutoARIMAX com season_length={ARIMAX_SEASON_LENGTH}. As features exógenas estão no DataFrame de entrada.\")\n",
    "# O método fit da StatsForecast recebe o DataFrame que já contém 'y' e as colunas exógenas.\n",
    "# O argumento X_df NÃO é usado no StatsForecast.fit()\n",
    "model_sf_arimax_to_save.fit(df_fit_arimax_with_exog) \n",
    "# ============================================================\n",
    "print(\"AutoARIMAX (10 anos com features) ajustado.\")\n",
    "\n",
    "model_sf_arimax_path_to_save = os.path.join(output_directory, 'sarima_model_sf.pkl')\n",
    "model_sf_arimax_to_save.save(model_sf_arimax_path_to_save)\n",
    "print(f\">>> Modelo AutoARIMAX (StatsForecast, 10 anos com features) salvo como '{model_sf_arimax_path_to_save}' <<<\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53927424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CÉLULA 5: Geração de Features ARIMAX para o LSTM Híbrido \n",
    "# ==============================================================================\n",
    "print(\"\\n--- Gerando Features ARIMAX para LSTM Híbrido (baseado no modelo de 10 anos) ---\")\n",
    "h_for_arimax_features_insample = len(df_fit_arimax_with_exog) \n",
    "\n",
    "# Para prever com o modelo ARIMAX treinado, precisamos fornecer as features exógenas\n",
    "# para o período da previsão. Neste caso, são as features históricas.\n",
    "# O X_df para o predict do StatsForecast deve conter 'unique_id', 'ds' e as colunas exógenas.\n",
    "X_df_for_predict_sf = df_fit_arimax_with_exog[['unique_id', 'ds'] + X_exog_arimax_unscaled_cols].copy()\n",
    "\n",
    "predicted_arimax_transformed_features = model_sf_arimax_to_save.predict(\n",
    "    h=h_for_arimax_features_insample, \n",
    "    X_df=X_df_for_predict_sf # Passa o DataFrame com as features exógenas históricas para o predict\n",
    ")\n",
    "\n",
    "# Reconstruir estas features ARIMAX para a escala original\n",
    "ma_log_series_aligned = df_transformed_10a_for_arimax.set_index('ds').loc[df_fit_arimax_with_exog['ds']]['y_ma_log']\n",
    "ma_log_relevant_arimax = ma_log_series_aligned.values\n",
    "\n",
    "len_preds_arimax = 30\n",
    "pred_col_name_arimax = 'AutoARIMA' \n",
    "if pred_col_name_arimax in predicted_arimax_transformed_features.columns:\n",
    "    len_preds_arimax = len(predicted_arimax_transformed_features[pred_col_name_arimax])\n",
    "    arimax_preds_values = predicted_arimax_transformed_features[pred_col_name_arimax].values\n",
    "elif not predicted_arimax_transformed_features.empty:\n",
    "    potential_pred_cols = [col for col in predicted_arimax_transformed_features.columns if col not in ['ds', 'unique_id']]\n",
    "    if potential_pred_cols:\n",
    "        pred_col_name_arimax = potential_pred_cols[0]\n",
    "        arimax_preds_values = predicted_arimax_transformed_features[pred_col_name_arimax].values\n",
    "        len_preds_arimax = len(arimax_preds_values)\n",
    "        print(f\"AVISO: Coluna 'AutoARIMA' não encontrada na previsão ARIMAX. Usando '{pred_col_name_arimax}'.\")\n",
    "    else: raise ValueError(\"Nenhuma coluna de previsão encontrada na saída do AutoARIMAX.\")\n",
    "else: raise ValueError(\"Previsão do AutoARIMAX resultou em DataFrame vazio.\")\n",
    "\n",
    "if len(ma_log_relevant_arimax) > len_preds_arimax:\n",
    "    ma_log_relevant_arimax = ma_log_relevant_arimax[-len_preds_arimax:]\n",
    "elif len(ma_log_relevant_arimax) < len_preds_arimax:\n",
    "    ma_log_relevant_arimax = np.pad(ma_log_relevant_arimax, \n",
    "                                   (0, len_preds_arimax - len(ma_log_relevant_arimax)),\n",
    "                                   'edge')\n",
    "features_arimax_orig_scale_10a = np.exp(\n",
    "    arimax_preds_values + ma_log_relevant_arimax\n",
    ")\n",
    "df_features_arimax_10a = pd.DataFrame({\n",
    "    'ds': predicted_arimax_transformed_features['ds'], \n",
    "    'y_arimax_feature': features_arimax_orig_scale_10a\n",
    "})\n",
    "print(f\"Features ARIMAX (10 anos) geradas e revertidas para escala original. Formato: {df_features_arimax_10a.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fea6a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CÉLULA 6: Preparação dos Dados Combinados e Engenharia de Features Sazonais para LSTM\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Preparando Dados Combinados e Features Sazonais para LSTM Híbrido (10 anos) ---\")\n",
    "pipeline_normalize_data_nb_10a = Pipeline([\n",
    "    ('data_prepator', PrepareData(date_col='Data', value_col='Value')),\n",
    "    ('filler_nan_values', FillNANValues(value_col='Value', new_value_col='y', new_date_col='ds'))])\n",
    "df_normalized_hist_10a_for_lstm = pipeline_normalize_data_nb_10a.fit_transform(df_brent_10anos_nb.copy())\n",
    "df_normalized_hist_10a_for_lstm['ds'] = pd.to_datetime(df_normalized_hist_10a_for_lstm['ds'])\n",
    "\n",
    "df_combinado_para_lstm_pre_feat = df_normalized_hist_10a_for_lstm.merge(\n",
    "    df_features_arimax_10a.rename(columns={'y_arimax_feature': 'y_arima_feature'}), on='ds', how='left'\n",
    ")\n",
    "df_combinado_para_lstm_pre_feat['y_final_lstm_input'] = df_combinado_para_lstm_pre_feat['y_arima_feature'].fillna(df_combinado_para_lstm_pre_feat['y'])\n",
    "df_combinado_para_lstm_final = df_combinado_para_lstm_pre_feat[['ds', 'y_final_lstm_input']].rename(columns={'y_final_lstm_input': 'y'})\n",
    "\n",
    "df_combinado_para_lstm_final['dia_da_semana'] = df_combinado_para_lstm_final['ds'].dt.dayofweek\n",
    "df_combinado_para_lstm_final['mes_sin'] = np.sin(2 * np.pi * df_combinado_para_lstm_final['ds'].dt.month / 12)\n",
    "df_combinado_para_lstm_final['mes_cos'] = np.cos(2 * np.pi * df_combinado_para_lstm_final['ds'].dt.month / 12)\n",
    "df_combinado_para_lstm_final['dia_do_ano_sin'] = np.sin(2 * np.pi * df_combinado_para_lstm_final['ds'].dt.dayofyear / 365.25)\n",
    "df_combinado_para_lstm_final['dia_do_ano_cos'] = np.cos(2 * np.pi * df_combinado_para_lstm_final['ds'].dt.dayofyear / 365.25)\n",
    "\n",
    "features_lstm_cols = ['y', 'dia_da_semana', 'mes_sin', 'mes_cos', 'dia_do_ano_sin', 'dia_do_ano_cos']\n",
    "df_para_scaler_lstm = df_combinado_para_lstm_final[features_lstm_cols].copy()\n",
    "df_para_scaler_lstm.dropna(inplace=True) \n",
    "if df_para_scaler_lstm.empty: raise ValueError(\"DataFrame para scaler LSTM vazio.\")\n",
    "print(f\"Dados combinados com features sazonais para LSTM. Formato: {df_para_scaler_lstm.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57be9ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CÉLULA 7: Normalização, Criação de Sequências e Treinamento do LSTM Híbrido\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Treinamento do Modelo LSTM Híbrido (10 anos com Features Sazonais) ---\")\n",
    "scaler_lstm_hibrido_10a_tuned = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data_hibrido_lstm_10a_tuned = scaler_lstm_hibrido_10a_tuned.fit_transform(df_para_scaler_lstm) \n",
    "scaler_lstm_path_10a_tuned = os.path.join(output_directory, 'scaler.pkl')\n",
    "joblib.dump(scaler_lstm_hibrido_10a_tuned, scaler_lstm_path_10a_tuned)\n",
    "print(f\">>> Scaler LSTM (Híbrido com Features, 10 anos) salvo como '{scaler_lstm_path_10a_tuned}' <<<\")\n",
    "\n",
    "seq_length_lstm_10a_tuned = 90 \n",
    "num_features_lstm = scaled_data_hibrido_lstm_10a_tuned.shape[1]\n",
    "print(f\"LSTM usará seq_length={seq_length_lstm_10a_tuned} e num_features={num_features_lstm}\")\n",
    "\n",
    "def create_sequences_multivariate_notebook(data, seq_length):\n",
    "    xs, ys = [], []\n",
    "    if len(data) <= seq_length: return np.array(xs), np.array(ys)\n",
    "    for i in range(len(data) - seq_length):\n",
    "        xs.append(data[i:(i + seq_length), :]) \n",
    "        ys.append(data[i + seq_length, 0])    \n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "X_hibrido_lstm_10a_tuned, y_hibrido_lstm_10a_tuned = create_sequences_multivariate_notebook(\n",
    "    scaled_data_hibrido_lstm_10a_tuned, seq_length_lstm_10a_tuned)\n",
    "if X_hibrido_lstm_10a_tuned.shape[0] == 0: raise ValueError(\"Nenhuma sequência LSTM multivariada criada.\")\n",
    "\n",
    "tam_validacao_lstm_10a = int(len(X_hibrido_lstm_10a_tuned) * 0.2) \n",
    "if tam_validacao_lstm_10a < 1 and len(X_hibrido_lstm_10a_tuned) > 1: tam_validacao_lstm_10a = 1\n",
    "elif len(X_hibrido_lstm_10a_tuned) <=1 : tam_validacao_lstm_10a = 0\n",
    "if len(X_hibrido_lstm_10a_tuned) - tam_validacao_lstm_10a <= 0 and len(X_hibrido_lstm_10a_tuned) > 0 :\n",
    "    tam_validacao_lstm_10a = 0 \n",
    "    print(\"AVISO: Poucos dados para validação LSTM, treinando com todos os dados de sequência.\")\n",
    "\n",
    "X_train_lstm_10a_tuned, X_val_lstm_10a_tuned = X_hibrido_lstm_10a_tuned[:-tam_validacao_lstm_10a] if tam_validacao_lstm_10a > 0 else X_hibrido_lstm_10a_tuned, \\\n",
    "                                             X_hibrido_lstm_10a_tuned[-tam_validacao_lstm_10a:] if tam_validacao_lstm_10a > 0 else np.array([])\n",
    "y_train_lstm_10a_tuned, y_val_lstm_10a_tuned = y_hibrido_lstm_10a_tuned[:-tam_validacao_lstm_10a] if tam_validacao_lstm_10a > 0 else y_hibrido_lstm_10a_tuned, \\\n",
    "                                             y_hibrido_lstm_10a_tuned[-tam_validacao_lstm_10a:] if tam_validacao_lstm_10a > 0 else np.array([])\n",
    "print(f\"Formato de X_train_lstm_tuned (multivariado): {X_train_lstm_10a_tuned.shape}\")\n",
    "if X_train_lstm_10a_tuned.shape[0] == 0: raise ValueError(\"X_train_lstm_10a_tuned está vazio.\")\n",
    "\n",
    "model_lstm_hibrido_10a_tuned = Sequential()\n",
    "model_lstm_hibrido_10a_tuned.add(LSTM(units=128, return_sequences=True, input_shape=(seq_length_lstm_10a_tuned, num_features_lstm)))\n",
    "model_lstm_hibrido_10a_tuned.add(Dropout(0.3))\n",
    "model_lstm_hibrido_10a_tuned.add(LSTM(units=128, return_sequences=False)) \n",
    "model_lstm_hibrido_10a_tuned.add(Dropout(0.3))\n",
    "model_lstm_hibrido_10a_tuned.add(Dense(64, activation='relu')) \n",
    "model_lstm_hibrido_10a_tuned.add(Dense(1))\n",
    "optimizer_10a_tuned = Adam(learning_rate=0.0005) \n",
    "model_lstm_hibrido_10a_tuned.compile(optimizer=optimizer_10a_tuned, loss='mean_squared_error')\n",
    "\n",
    "print(\"\\nIniciando treinamento do modelo LSTM híbrido com features sazonais (10 anos)...\")\n",
    "epochs_10a_tuned = 200 \n",
    "batch_size_10a_tuned = 80 \n",
    "early_stopping_10a = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=1)\n",
    "validation_data_lstm_10a = (X_val_lstm_10a_tuned, y_val_lstm_10a_tuned) if X_val_lstm_10a_tuned.shape[0] > 0 else None\n",
    "history_lstm_10a_tuned = model_lstm_hibrido_10a_tuned.fit(\n",
    "    X_train_lstm_10a_tuned, y_train_lstm_10a_tuned, epochs=epochs_10a_tuned, batch_size=batch_size_10a_tuned, \n",
    "    validation_data=validation_data_lstm_10a, callbacks=[early_stopping_10a] if validation_data_lstm_10a else [], verbose=1)\n",
    "print(\"Treinamento LSTM (10 anos com features) concluído.\")\n",
    "\n",
    "model_lstm_h5_path_10a_tuned = os.path.join(output_directory, 'lstm_model.h5')\n",
    "model_lstm_hibrido_10a_tuned.save(model_lstm_h5_path_10a_tuned)\n",
    "print(f\">>> Modelo LSTM híbrido (10 anos com features) salvo como '{model_lstm_h5_path_10a_tuned}' <<<\")\n",
    "\n",
    "if validation_data_lstm_10a and 'val_loss' in history_lstm_10a_tuned.history:\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(history_lstm_10a_tuned.history['loss'], label='Perda de Treino')\n",
    "    if 'val_loss' in history_lstm_10a_tuned.history:\n",
    "        plt.plot(history_lstm_10a_tuned.history['val_loss'], label='Perda de Validação')\n",
    "    plt.title('Perda do Modelo LSTM Ajustado (10 anos com Features)')\n",
    "    plt.xlabel('Época'); plt.ylabel('Perda'); plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae92338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CÉLULA 8: Avaliação (Exemplo) e Salvamento de df_erros.csv\n",
    "# ==============================================================================\n",
    "calculos_erro_list_notebook = [] \n",
    "if validation_data_lstm_10a and X_val_lstm_10a_tuned.shape[0] > 0:\n",
    "    pred_val_lstm_scaled_10a = model_lstm_hibrido_10a_tuned.predict(X_val_lstm_10a_tuned)\n",
    "    dummy_array_for_inverse = np.zeros((len(pred_val_lstm_scaled_10a), num_features_lstm))\n",
    "    dummy_array_for_inverse[:, 0] = pred_val_lstm_scaled_10a.flatten()\n",
    "    pred_val_lstm_original_scale_10a = scaler_lstm_hibrido_10a_tuned.inverse_transform(dummy_array_for_inverse)[:, 0]\n",
    "    dummy_array_for_inverse_y_true = np.zeros((len(y_val_lstm_10a_tuned), num_features_lstm))\n",
    "    dummy_array_for_inverse_y_true[:, 0] = y_val_lstm_10a_tuned.flatten()\n",
    "    y_val_original_scale_10a = scaler_lstm_hibrido_10a_tuned.inverse_transform(dummy_array_for_inverse_y_true)[:, 0]\n",
    "    print(\"\\nMétricas para LSTM Híbrido (10 anos com Features) no conjunto de validação:\")\n",
    "    print(calcula_erro_notebook(pred_val_lstm_original_scale_10a, y_val_original_scale_10a, \"LSTM_Hibrido_ feats_10Anos_Val\"))\n",
    "\n",
    "df_erros_path_notebook = os.path.join(output_directory, 'df_erros.csv')\n",
    "if calculos_erro_list_notebook:\n",
    "    df_erros_final_notebook = pd.DataFrame(calculos_erro_list_notebook)\n",
    "    df_erros_final_notebook.to_csv(df_erros_path_notebook, index=False)\n",
    "    print(f\">>> Métricas de erro salvas como '{df_erros_path_notebook}' <<<\")\n",
    "    print(\"\\nDataFrame de Erros (do Notebook):\\n\", df_erros_final_notebook)\n",
    "else:\n",
    "    print(f\"AVISO: 'calculos_erro_list_notebook' não populada. '{df_erros_path_notebook}' não foi criado.\")\n",
    "\n",
    "print(\"\\n--- Processo de Geração de Artefatos (10 anos, ARIMAX e LSTM com features) Concluído ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
